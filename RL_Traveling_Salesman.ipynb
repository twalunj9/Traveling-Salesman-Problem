{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "RL Traveling Salesman.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmHYD6lT1W8-"
      },
      "source": [
        "# RL Traveling Salesman\n",
        "\n",
        "In this notebook, we implement a reinforcement learning approach to the traveling salesman problem. It is based on Q-learning and structure2vec graph embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrAtMEfb1YSx"
      },
      "source": [
        "19i190003 Shubham Chaudhary,\n",
        "19i190001 Tushar Shankar Walunj,\n",
        "19i190007 Reena Meena,\n",
        "19i190004 Neeraj Kumar bhargava,\n",
        "19i190009 Shubham. \n",
        "Code Source: https://github.com/unit8co/medium-tsp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ach2woJy1W8-",
        "outputId": "491e5cfa-80dc-4e93-a552-d0c5df4e9bae"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "from collections import namedtuple\n",
        "import os\n",
        "import time\n",
        "\n",
        "from scipy.spatial import distance_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from scipy.signal import medfilt\n",
        "\n",
        "\"\"\" Note: the code is not optimized for GPU\n",
        "\"\"\"\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-b11ea6659da5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2yhFwdQ1W8-"
      },
      "source": [
        "## Create Graph Instances\n",
        "Below we implement a small method to generate random graph instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF4AbISu1W8-"
      },
      "source": [
        "def get_graph_mat(n=10, size=1):\n",
        "    \"\"\" Throws n nodes uniformly at random on a square, and build a (fully connected) graph.\n",
        "        Returns the (N, 2) coordinates matrix, and the (N, N) matrix containing pairwise euclidean distances.\n",
        "    \"\"\"\n",
        "    coords = size * np.random.uniform(size=(n,2))\n",
        "    dist_mat = distance_matrix(coords, coords)\n",
        "    return coords, dist_mat\n",
        "\n",
        "def plot_graph(coords, mat):\n",
        "    \"\"\" Utility function to plot the fully connected graph\n",
        "    \"\"\"\n",
        "    n = len(coords)\n",
        "    \n",
        "    plt.scatter(coords[:,0], coords[:,1], s=[50 for _ in range(n)])\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if j < i:\n",
        "                plt.plot([coords[i,0], coords[j,0]], [coords[i,1], coords[j,1]], 'b', alpha=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiEzQIzS1W8_"
      },
      "source": [
        "coords, W_np = get_graph_mat(n=10)\n",
        "plot_graph(coords, W_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHoAwx0U1W8_"
      },
      "source": [
        "## Define the state\n",
        "We now define the state tuple, containing a graph (given by a weights matrix `W`), the noode coordinates `coords` and the partial solution (list of visited nodes).\n",
        "We also define the function `state2tens`, which translates such tuples into tensors (partially loosing the sequence order information)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jlqj8Hz1W8_"
      },
      "source": [
        "State = namedtuple('State', ('W', 'coords', 'partial_solution'))\n",
        "  \n",
        "def state2tens(state):\n",
        "    \"\"\" Creates a Pytorch tensor representing the history of visited nodes, from a (single) state tuple.\n",
        "        \n",
        "        Returns a (Nx5) tensor, where for each node we store whether this node is in the sequence,\n",
        "        whether it is first or last, and its (x,y) coordinates.\n",
        "    \"\"\"\n",
        "    solution = set(state.partial_solution)\n",
        "    sol_last_node = state.partial_solution[-1] if len(state.partial_solution) > 0 else -1\n",
        "    sol_first_node = state.partial_solution[0] if len(state.partial_solution) > 0 else -1\n",
        "    coords = state.coords\n",
        "    nr_nodes = coords.shape[0]\n",
        "\n",
        "    xv = [[(1 if i in solution else 0),(1 if i == sol_first_node else 0),(1 if i == sol_last_node else 0),coords[i,0],coords[i,1]] for i in range(nr_nodes)]\n",
        "    \n",
        "    return torch.tensor(xv, dtype=torch.float32, requires_grad=False, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_hfonP71W8_"
      },
      "source": [
        "## The Q-Function\n",
        "Below, we write the neural network that will parameterize the function Q(s, a)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oGRU2c11W8_"
      },
      "source": [
        "class QNet(nn.Module):\n",
        "    \"\"\" The neural net that will parameterize the function Q(s, a)\n",
        "    \n",
        "        The input is the state (containing the graph and visited nodes),\n",
        "        and the output is a vector of size N containing Q(s, a) for each of the N actions a.\n",
        "    \"\"\"    \n",
        "    \n",
        "    def __init__(self, emb_dim, T=4):\n",
        "        \"\"\" emb_dim: embedding dimension p\n",
        "            T: number of iterations for the graph embedding\n",
        "        \"\"\"\n",
        "        super(QNet, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.T = T\n",
        "        \n",
        "        # We use 5 dimensions for representing the nodes' states:\n",
        "        # * A binary variable indicating whether the node has been visited\n",
        "        # * A binary variable indicating whether the node is the first of the visited sequence\n",
        "        # * A binary variable indicating whether the node is the last of the visited sequence\n",
        "        # * The (x, y) coordinates of the node.\n",
        "        self.node_dim = 5\n",
        "        \n",
        "        # We can have an extra layer after theta_1 (for the sake of example to make the network deeper)\n",
        "        nr_extra_layers_1 = 1\n",
        "        \n",
        "        # Build the learnable affine maps:\n",
        "        self.theta1 = nn.Linear(self.node_dim, self.emb_dim, True)\n",
        "        self.theta2 = nn.Linear(self.emb_dim, self.emb_dim, True)\n",
        "        self.theta3 = nn.Linear(self.emb_dim, self.emb_dim, True)\n",
        "        self.theta4 = nn.Linear(1, self.emb_dim, True)\n",
        "        self.theta5 = nn.Linear(2*self.emb_dim, 1, True)\n",
        "        self.theta6 = nn.Linear(self.emb_dim, self.emb_dim, True)\n",
        "        self.theta7 = nn.Linear(self.emb_dim, self.emb_dim, True)\n",
        "        \n",
        "        self.theta1_extras = [nn.Linear(self.emb_dim, self.emb_dim, True) for _ in range(nr_extra_layers_1)]\n",
        "        \n",
        "    def forward(self, xv, Ws):\n",
        "        # xv: The node features (batch_size, num_nodes, node_dim)\n",
        "        # Ws: The graphs (batch_size, num_nodes, num_nodes)\n",
        "        \n",
        "        num_nodes = xv.shape[1]\n",
        "        batch_size = xv.shape[0]\n",
        "        \n",
        "        # pre-compute 1-0 connection matrices masks (batch_size, num_nodes, num_nodes)\n",
        "        conn_matrices = torch.where(Ws > 0, torch.ones_like(Ws), torch.zeros_like(Ws)).to(device)\n",
        "        \n",
        "        # Graph embedding\n",
        "        # Note: we first compute s1 and s3 once, as they are not dependent on mu\n",
        "        mu = torch.zeros(batch_size, num_nodes, self.emb_dim, device=device)\n",
        "        s1 = self.theta1(xv)  # (batch_size, num_nodes, emb_dim)\n",
        "        for layer in self.theta1_extras:\n",
        "            s1 = layer(F.relu(s1))  # we apply the extra layer\n",
        "        \n",
        "        s3_1 = F.relu(self.theta4(Ws.unsqueeze(3)))  # (batch_size, nr_nodes, nr_nodes, emb_dim) - each \"weigth\" is a p-dim vector        \n",
        "        s3_2 = torch.sum(s3_1, dim=1)  # (batch_size, nr_nodes, emb_dim) - the embedding for each node\n",
        "        s3 = self.theta3(s3_2)  # (batch_size, nr_nodes, emb_dim)\n",
        "        \n",
        "        for t in range(self.T):\n",
        "            s2 = self.theta2(conn_matrices.matmul(mu))    \n",
        "            mu = F.relu(s1 + s2 + s3)\n",
        "            \n",
        "        \"\"\" prediction\n",
        "        \"\"\"\n",
        "        # we repeat the global state (summed over nodes) for each node, \n",
        "        # in order to concatenate it to local states later\n",
        "        global_state = self.theta6(torch.sum(mu, dim=1, keepdim=True).repeat(1, num_nodes, 1))\n",
        "        \n",
        "        local_action = self.theta7(mu)  # (batch_dim, nr_nodes, emb_dim)\n",
        "            \n",
        "        out = F.relu(torch.cat([global_state, local_action], dim=2))\n",
        "        return self.theta5(out).squeeze(dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXcsrkU41W8_"
      },
      "source": [
        "\"\"\" See what the model returns\n",
        "\"\"\"\n",
        "model = QNet(3, T=1).to(device)\n",
        "coords, W_np = get_graph_mat(n=10)\n",
        "W = torch.tensor(W_np, dtype=torch.float32, device=device)\n",
        "xv = torch.rand((1, W.shape[0], 5)).to(device) # random node state\n",
        "Ws = W.unsqueeze(0)\n",
        "\n",
        "y = model(xv, Ws)\n",
        "print('model output: {}'.format(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU82zotv1W8_"
      },
      "source": [
        "### A wrapper around the neural net\n",
        "Below, we define the class `QFunction`, which will act as a wrapper around the neural net `QNet`, and can manipulate TSP (partial) solutions and translate them to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM2FObbl1W8_"
      },
      "source": [
        "class QFunction():\n",
        "    def __init__(self, model, optimizer, lr_scheduler):\n",
        "        self.model = model  # The actual QNet\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "    \n",
        "    def predict(self, state_tsr, W):\n",
        "        # batch of 1 - only called at inference time\n",
        "        with torch.no_grad():\n",
        "            estimated_rewards = self.model(state_tsr.unsqueeze(0), W.unsqueeze(0))\n",
        "        return estimated_rewards[0]\n",
        "                \n",
        "    def get_best_action(self, state_tsr, state):\n",
        "        \"\"\" Computes the best (greedy) action to take from a given state\n",
        "            Returns a tuple containing the ID of the next node and the corresponding estimated reward\n",
        "        \"\"\"\n",
        "        W = state.W\n",
        "        estimated_rewards = self.predict(state_tsr, W)  # size (nr_nodes,)\n",
        "        sorted_reward_idx = estimated_rewards.argsort(descending=True)\n",
        "        \n",
        "        solution = state.partial_solution\n",
        "        \n",
        "        already_in = set(solution)\n",
        "        for idx in sorted_reward_idx.tolist():\n",
        "            if (len(solution) == 0 or W[solution[-1], idx] > 0) and idx not in already_in:\n",
        "                return idx, estimated_rewards[idx].item()\n",
        "        \n",
        "    def batch_update(self, states_tsrs, Ws, actions, targets):\n",
        "        \"\"\" Take a gradient step using the loss computed on a batch of (states, Ws, actions, targets)\n",
        "        \n",
        "            states_tsrs: list of (single) state tensors\n",
        "            Ws: list of W tensors\n",
        "            actions: list of actions taken\n",
        "            targets: list of targets (resulting estimated rewards after taking the actions)\n",
        "        \"\"\"        \n",
        "        Ws_tsr = torch.stack(Ws).to(device)\n",
        "        xv = torch.stack(states_tsrs).to(device)\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # the rewards estimated by Q for the given actions\n",
        "        estimated_rewards = self.model(xv, Ws_tsr)[range(len(actions)), actions]\n",
        "        \n",
        "        loss = self.loss_fn(estimated_rewards, torch.tensor(targets, device=device))\n",
        "        loss_val = loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()        \n",
        "        self.lr_scheduler.step()\n",
        "        \n",
        "        return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTeFmI8g1W8_"
      },
      "source": [
        "## Define experiences & memory\n",
        "We'll now a tuple representing an experience, and the memory that contains such experiences.\n",
        "An experience is composed of a (state, action) tuple, and the corresponding \"next state\" and reward. The \"next state\" can be N step after the \"state\" in the case of N-step Q-learning. In experiences, we save states both in their tuple and tensor representations, in order to avoid computing these somewhat expensive translations after the experience has been stored.\n",
        "\n",
        "The `Memory` class implements a memory of limited size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpkaVMlo1W8_",
        "outputId": "29c4de8d-ace8-459c-a56b-2059f968fae7"
      },
      "source": [
        "# Note: we store state tensors in experience to compute these tensors only once later on\n",
        "Experience = namedtuple('Experience', ('state', 'state_tsr', 'action', 'reward', 'next_state', 'next_state_tsr'))\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "        self.nr_inserts = 0\n",
        "        \n",
        "    def remember(self, experience):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = experience\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "        self.nr_inserts += 1\n",
        "        \n",
        "    def sample_batch(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return min(self.nr_inserts, self.capacity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'namedtuple' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-df0e9e03e478>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Note: we store state tensors in experience to compute these tensors only once later on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mExperience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Experience'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'state_tsr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'action'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reward'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'next_state'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'next_state_tsr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mMemory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'namedtuple' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPmLR3Kn1W8_"
      },
      "source": [
        "## Other useful things\n",
        "Below, we'll write a few more helper functions for computing solution length, deciding if a state is final and getting a next node at random from the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64dVtQ5i1W8_"
      },
      "source": [
        "def total_distance(solution, W):\n",
        "    if len(solution) < 2:\n",
        "        return 0  # there is no travel\n",
        "    \n",
        "    total_dist = 0\n",
        "    for i in range(len(solution) - 1):\n",
        "        total_dist += W[solution[i], solution[i+1]].item()\n",
        "        \n",
        "    # if this solution is \"complete\", go back to initial point\n",
        "    if len(solution) == W.shape[0]:\n",
        "        total_dist += W[solution[-1], solution[0]].item()\n",
        "\n",
        "    return total_dist\n",
        "        \n",
        "def is_state_final(state):\n",
        "    return len(set(state.partial_solution)) == state.W.shape[0]\n",
        "\n",
        "def get_next_neighbor_random(state):\n",
        "    solution, W = state.partial_solution, state.W\n",
        "    \n",
        "    if len(solution) == 0:\n",
        "        return random.choice(range(W.shape[0]))\n",
        "    already_in = set(solution)\n",
        "    candidates = list(filter(lambda n: n.item() not in already_in, W[solution[-1]].nonzero()))\n",
        "    if len(candidates) == 0:\n",
        "        return None\n",
        "    return random.choice(candidates).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15XONFkV1W8_"
      },
      "source": [
        "## Training Code\n",
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gjnOqZc1W8_"
      },
      "source": [
        "SEED = 1  # A seed for the random number generator\n",
        "\n",
        "# Graph\n",
        "NR_NODES = 10  # Number of nodes N\n",
        "EMBEDDING_DIMENSIONS = 5  # Embedding dimension D\n",
        "EMBEDDING_ITERATIONS_T = 1  # Number of embedding iterations T\n",
        "\n",
        "# Learning\n",
        "NR_EPISODES = 4001\n",
        "MEMORY_CAPACITY = 10000\n",
        "N_STEP_QL = 2  # Number of steps (n) in n-step Q-learning to wait before computing target reward estimate\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "GAMMA = 0.9\n",
        "INIT_LR = 5e-3\n",
        "LR_DECAY_RATE = 1. - 2e-5  # learning rate decay\n",
        "\n",
        "MIN_EPSILON = 0.1\n",
        "EPSILON_DECAY_RATE = 6e-4  # epsilon decay\n",
        "\n",
        "FOLDER_NAME = './models'  # where to checkpoint the best models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP2y9dbR1W8_"
      },
      "source": [
        "### Model Initialization & checkpointing\n",
        "We will save our best models over the course of training - here we just define two functions for loading and checkpointing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_v8KxtA1W8_"
      },
      "source": [
        "def init_model(fname=None):\n",
        "    \"\"\" Create a new model. If fname is defined, load the model from the specified file.\n",
        "    \"\"\"\n",
        "    Q_net = QNet(EMBEDDING_DIMENSIONS, T=EMBEDDING_ITERATIONS_T).to(device)\n",
        "    optimizer = optim.Adam(Q_net.parameters(), lr=INIT_LR)\n",
        "    lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_DECAY_RATE)\n",
        "    \n",
        "    if fname is not None:\n",
        "        checkpoint = torch.load(fname)\n",
        "        Q_net.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "    \n",
        "    Q_func = QFunction(Q_net, optimizer, lr_scheduler)\n",
        "    return Q_func, Q_net, optimizer, lr_scheduler\n",
        "\n",
        "def checkpoint_model(model, optimizer, lr_scheduler, loss, \n",
        "                     episode, avg_length):\n",
        "    if not os.path.exists(FOLDER_NAME):\n",
        "        os.makedirs(FOLDER_NAME)\n",
        "    \n",
        "    fname = os.path.join(FOLDER_NAME, 'ep_{}'.format(episode))\n",
        "    fname += '_length_{}'.format(avg_length)\n",
        "    fname += '.tar'\n",
        "    \n",
        "    torch.save({\n",
        "        'episode': episode,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'lr_scheduler': lr_scheduler.state_dict(),\n",
        "        'loss': loss,\n",
        "        'avg_length': avg_length\n",
        "    }, fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys6EZ9C91W8_"
      },
      "source": [
        "### Training Loop\n",
        "The code below is slightly more complex than the version appearing in the blog article. Notably, here in several places we maintain states represented both as tuples and tensors. We cache the tensor versions in order to avoid re-computing them many times. Also different from the article, the version below implements a version of n-step Q-learning, it checkpoints the best models (according to the median path length), and prints some information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlND4Kpl1W8_",
        "outputId": "01366321-0bb5-4160-af58-389fb7e2a10d"
      },
      "source": [
        "# seed everything for reproducible results first:\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Create module, optimizer, LR scheduler, and Q-function\n",
        "Q_func, Q_net, optimizer, lr_scheduler = init_model()\n",
        "\n",
        "# Create memory\n",
        "memory = Memory(MEMORY_CAPACITY)\n",
        "\n",
        "# Storing metrics about training:\n",
        "found_solutions = dict()  # episode --> (coords, W, solution)\n",
        "losses = []\n",
        "path_lengths = []\n",
        "\n",
        "# keep track of median path length for model checkpointing\n",
        "current_min_med_length = float('inf')\n",
        "\n",
        "for episode in range(NR_EPISODES):\n",
        "    # sample a new random graph\n",
        "    coords, W_np = get_graph_mat(n=NR_NODES)\n",
        "    W = torch.tensor(W_np, dtype=torch.float32, requires_grad=False, device=device)\n",
        "    \n",
        "    # current partial solution - a list of node index\n",
        "    solution = [random.randint(0, NR_NODES-1)]\n",
        "    \n",
        "    # current state (tuple and tensor)\n",
        "    current_state = State(partial_solution=solution, W=W, coords=coords)\n",
        "    current_state_tsr = state2tens(current_state)\n",
        "    \n",
        "    # Keep track of some variables for insertion in replay memory:\n",
        "    states = [current_state]\n",
        "    states_tsrs = [current_state_tsr]  # we also keep the state tensors here (for efficiency)\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    \n",
        "    # current value of epsilon\n",
        "    epsilon = max(MIN_EPSILON, (1-EPSILON_DECAY_RATE)**episode)\n",
        "    \n",
        "    nr_explores = 0\n",
        "    t = -1\n",
        "    while not is_state_final(current_state):\n",
        "        t += 1  # time step of this episode\n",
        "        \n",
        "        if epsilon >= random.random():\n",
        "            # explore\n",
        "            next_node = get_next_neighbor_random(current_state)\n",
        "            nr_explores += 1\n",
        "        else:\n",
        "            # exploit\n",
        "            next_node, est_reward = Q_func.get_best_action(current_state_tsr, current_state)\n",
        "            if episode % 50 == 0:\n",
        "                print('Ep {} | current sol: {} / next est reward: {}'.format(episode, solution, est_reward))\n",
        "        \n",
        "        next_solution = solution + [next_node]\n",
        "        \n",
        "        # reward observed for taking this step        \n",
        "        reward = -(total_distance(next_solution, W) - total_distance(solution, W))\n",
        "        \n",
        "        next_state = State(partial_solution=next_solution, W=W, coords=coords)\n",
        "        next_state_tsr = state2tens(next_state)\n",
        "        \n",
        "        # store rewards and states obtained along this episode:\n",
        "        states.append(next_state)\n",
        "        states_tsrs.append(next_state_tsr)\n",
        "        rewards.append(reward)\n",
        "        actions.append(next_node)\n",
        "        \n",
        "        # store our experience in memory, using n-step Q-learning:\n",
        "        if len(solution) >= N_STEP_QL:\n",
        "            memory.remember(Experience(state=states[-N_STEP_QL],\n",
        "                                       state_tsr=states_tsrs[-N_STEP_QL],\n",
        "                                       action=actions[-N_STEP_QL],\n",
        "                                       reward=sum(rewards[-N_STEP_QL:]),\n",
        "                                       next_state=next_state,\n",
        "                                       next_state_tsr=next_state_tsr))\n",
        "            \n",
        "        if is_state_final(next_state):\n",
        "            for n in range(1, N_STEP_QL):\n",
        "                memory.remember(Experience(state=states[-n],\n",
        "                                           state_tsr=states_tsrs[-n], \n",
        "                                           action=actions[-n], \n",
        "                                           reward=sum(rewards[-n:]), \n",
        "                                           next_state=next_state,\n",
        "                                           next_state_tsr=next_state_tsr))\n",
        "        \n",
        "        # update state and current solution\n",
        "        current_state = next_state\n",
        "        current_state_tsr = next_state_tsr\n",
        "        solution = next_solution\n",
        "        \n",
        "        # take a gradient step\n",
        "        loss = None\n",
        "        if len(memory) >= BATCH_SIZE and len(memory) >= 2000:\n",
        "            experiences = memory.sample_batch(BATCH_SIZE)\n",
        "            \n",
        "            batch_states_tsrs = [e.state_tsr for e in experiences]\n",
        "            batch_Ws = [e.state.W for e in experiences]\n",
        "            batch_actions = [e.action for e in experiences]\n",
        "            batch_targets = []\n",
        "            \n",
        "            for i, experience in enumerate(experiences):\n",
        "                target = experience.reward\n",
        "                if not is_state_final(experience.next_state):\n",
        "                    _, best_reward = Q_func.get_best_action(experience.next_state_tsr, \n",
        "                                                            experience.next_state)\n",
        "                    target += GAMMA * best_reward\n",
        "                batch_targets.append(target)\n",
        "                \n",
        "            # print('batch targets: {}'.format(batch_targets))\n",
        "            loss = Q_func.batch_update(batch_states_tsrs, batch_Ws, batch_actions, batch_targets)\n",
        "            losses.append(loss)\n",
        "            \n",
        "            \"\"\" Save model when we reach a new low average path length\n",
        "            \"\"\"\n",
        "            med_length = np.median(path_lengths[-100:])\n",
        "            if med_length < current_min_med_length:\n",
        "                current_min_med_length = med_length\n",
        "                checkpoint_model(Q_net, optimizer, lr_scheduler, loss, episode, med_length)\n",
        "                \n",
        "    length = total_distance(solution, W)\n",
        "    path_lengths.append(length)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print('Ep %d. Loss = %.3f / median length = %.3f / last = %.4f / epsilon = %.4f / lr = %.4f' % (\n",
        "            episode, (-1 if loss is None else loss), np.median(path_lengths[-50:]), length, epsilon,\n",
        "            Q_func.optimizer.param_groups[0]['lr']))\n",
        "        found_solutions[episode] = (W.clone(), coords.copy(), [n for n in solution])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-6-efb58b8fe207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# seed everything for reproducible results first:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5sD5Gl61W8_"
      },
      "source": [
        "## Inspect Training Metrics\n",
        "Let's look at the (moving averages of) loss and path length over training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxWDkiHx1W8_",
        "outputId": "19de3060-93af-48c0-9ab2-66884145fcc6"
      },
      "source": [
        "def _moving_avg(x, N=10):\n",
        "    return np.convolve(np.array(x), np.ones((N,))/N, mode='valid')\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.semilogy(_moving_avg(losses, 100))\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('training iteration')\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(_moving_avg(path_lengths, 100))\n",
        "plt.ylabel('average length')\n",
        "plt.xlabel('episode')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-7-bf79b1550e71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_moving_avg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWWJMTGx1W8_"
      },
      "source": [
        "## Re-run Best Model and Look at Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cWgRIwt1W9A",
        "outputId": "5bc00d1f-285e-433c-ec83-51366b280a4c"
      },
      "source": [
        "\"\"\" Get file with smallest distance\n",
        "\"\"\"\n",
        "all_lengths_fnames = [f for f in os.listdir(FOLDER_NAME) if f.endswith('.tar')]\n",
        "shortest_fname = sorted(all_lengths_fnames, key=lambda s: float(s.split('.tar')[0].split('_')[-1]))[0]\n",
        "print('shortest avg length found: {}'.format(shortest_fname.split('.tar')[0].split('_')[-1]))\n",
        "\n",
        "\"\"\" Load checkpoint\n",
        "\"\"\"\n",
        "Q_func, Q_net, optimizer, lr_scheduler = init_model(os.path.join(FOLDER_NAME, shortest_fname))\n",
        "\n",
        "\"\"\" A function to plot solutions\n",
        "\"\"\"\n",
        "def plot_solution(coords, mat, solution):\n",
        "    plt.scatter(coords[:,0], coords[:,1])\n",
        "    n = len(coords)\n",
        "    \n",
        "    for idx in range(n-1):\n",
        "        i, next_i = solution[idx], solution[idx+1]\n",
        "        plt.plot([coords[i, 0], coords[next_i, 0]], [coords[i, 1], coords[next_i, 1]], 'k', lw=2, alpha=0.8)\n",
        "    \n",
        "    i, next_i = solution[-1], solution[0]\n",
        "    plt.plot([coords[i, 0], coords[next_i, 0]], [coords[i, 1], coords[next_i, 1]], 'k', lw=2, alpha=0.8)\n",
        "    plt.plot(coords[solution[0], 0], coords[solution[0], 1], 'x', markersize=10)\n",
        "\n",
        "\n",
        "    \n",
        "\"\"\" Generate example solutions\n",
        "\"\"\"\n",
        "NR_NODES = 10\n",
        "for sample in range(10):\n",
        "    coords, W_np = get_graph_mat(n=NR_NODES)\n",
        "    W = torch.tensor(W_np, dtype=torch.float32, requires_grad=False, device=device)\n",
        "    \n",
        "    solution = [random.randint(0, NR_NODES-1)]\n",
        "    current_state = State(partial_solution=solution, W=W, coords=coords)\n",
        "    current_state_tsr = state2tens(current_state)\n",
        "    \n",
        "    while not is_state_final(current_state):\n",
        "        next_node, est_reward = Q_func.get_best_action(current_state_tsr, \n",
        "                                                       current_state)\n",
        "        \n",
        "        \n",
        "        solution = solution + [next_node]\n",
        "        current_state = State(partial_solution=solution, W=W, coords=coords)\n",
        "        current_state_tsr = state2tens(current_state)\n",
        "        \n",
        "    plt.figure()\n",
        "    plot_solution(coords, W, solution)\n",
        "    plt.title('model / len = {}'.format(total_distance(solution, W)))\n",
        "    \n",
        "    # for comparison, plot a random solution\n",
        "    plt.figure()\n",
        "    random_solution = list(range(NR_NODES))\n",
        "    plot_solution(coords, W, random_solution)\n",
        "    plt.title('random / len = {}'.format(total_distance(random_solution, W)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-54a3d1f14d7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \"\"\" Get file with smallest distance\n\u001b[0;32m      2\u001b[0m \"\"\"\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mall_lengths_fnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFOLDER_NAME\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.tar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mshortest_fname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_lengths_fnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.tar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shortest avg length found: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshortest_fname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.tar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTh7ONej1W9A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByFHREuO1W9A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RphGu2w1W9A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e4RUvV11W9A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}